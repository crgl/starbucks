# Starbucks Data
## Or, boldly regressing on categorical data

Here is a fairly common problem: You have available to you some intervention (say, an email or a coupon), with some small fixed cost per person which will (ostensibly) increase the probability of a customer making a one-time purchase. There is strong evidence of the efficacy of the intervention, but without any targeting you're making a net loss. This works out pretty neatly when it comes to financial interventions because everything shakes out to dollars, but targeting an intervention to a subgroup where the positives outweigh the negatives is pretty general.

In this case study, we are given a variety of features, a binary variable representing whether or not a promotion was introduced to the customer, and another binary variable representing whether or not that customer made a purchase. Because the variable of interest is categorical, there may be some initial instinct to use classifiers. Indeed, classifiers can often output probabilities, and we are most interested in comparing the probability of purchase with and without intervention. Still, I used linear regression rather than logistic regression. What gives?

Logistic regression has some real benefits, when it comes to talking about probability. For one, it produces values that are valid probabilities. For another, people are often not pleased with you when you try to use OLS for data even though you know the residuals can't be normally distributed. Nevertheless, we press on! We want an estimate of the percentage chance of purchase, which is equivalent to the fraction of 1s, which is equivalent to the mean of the categorical variable represented as 1s and 0s. In this case, we're actually interested in the difference in these values between two groups. Minimizing the mean squared error provides an estimate of this mean, just as minimizing log loss in logistic regression provides an estimate of the probability. Each relies on a different model of the data. I initally used linear regression more on a lark than anything, but it performs well in cross validation and ultimately on testing data.

Now, some people will point out that OLS is obviously wrong, in that the underlying model can't be true. These accusations, if true, are concerning! After all, if, say, income has a linear effect on the probability of making a one-time purchase, then our model may predict that some millionaire or billionaire might make two one-time purchases! To that I can only say: ask me about my second home. Seriously, I modeled rate of soccer goals once as a constant plus elapsed time multiplied by a constant factor. That's simplistic and clearly wrong: there is a trivial maximum rate of scoring goals in reality while the model treats that rate as unbounded. Similarly, if you're talking about the speed of a baseball you can usually get away with ignoring relativity (and even the rotation of the Earth).

More succinctly, all models are wrong. I particularly like OLS for this because whatever the individual predictions, the difference comes out comfortably on the order of probabilities. In addition, I'm just cutting things off at a threshold! I chose my threshold based on the interpretation as a probability, but it's a flat area of the curve. I could choose it instead based on properties of the training data, and then I'm just building a small decision tree on a linear combination of features. I could do LinearSVC with a soft margin in 2D using the two models, but if a relatively simple model works well, I'm all for it. A threshold based on vaguely sound theory on the one-dimensional output of a linear model is just right.

In the brief time that this is up while the notebook is being cleaned, I hope no one is too bothered by this hot take on OLS. I don't really have a dog in that fight, yet, I've just been surprised and pleased by how well it performed here.
